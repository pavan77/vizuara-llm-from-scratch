{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2934cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecce39fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c970d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1547a61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2412c406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1821c290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb6f548d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b352857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cece91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n",
      "('His', 51)\n",
      "('How', 52)\n",
      "('I', 53)\n",
      "('If', 54)\n",
      "('In', 55)\n",
      "('It', 56)\n",
      "('Jack', 57)\n",
      "('Jove', 58)\n",
      "('Just', 59)\n",
      "('Lord', 60)\n",
      "('Made', 61)\n",
      "('Miss', 62)\n",
      "('Money', 63)\n",
      "('Monte', 64)\n",
      "('Moon-dancers', 65)\n",
      "('Mr', 66)\n",
      "('Mrs', 67)\n",
      "('My', 68)\n",
      "('Never', 69)\n",
      "('No', 70)\n",
      "('Now', 71)\n",
      "('Nutley', 72)\n",
      "('Of', 73)\n",
      "('Oh', 74)\n",
      "('On', 75)\n",
      "('Once', 76)\n",
      "('Only', 77)\n",
      "('Or', 78)\n",
      "('Perhaps', 79)\n",
      "('Poor', 80)\n",
      "('Professional', 81)\n",
      "('Renaissance', 82)\n",
      "('Rickham', 83)\n",
      "('Riviera', 84)\n",
      "('Rome', 85)\n",
      "('Russian', 86)\n",
      "('Sevres', 87)\n",
      "('She', 88)\n",
      "('Stroud', 89)\n",
      "('Strouds', 90)\n",
      "('Suddenly', 91)\n",
      "('That', 92)\n",
      "('The', 93)\n",
      "('Then', 94)\n",
      "('There', 95)\n",
      "('They', 96)\n",
      "('This', 97)\n",
      "('Those', 98)\n",
      "('Though', 99)\n",
      "('Thwing', 100)\n",
      "('Thwings', 101)\n",
      "('To', 102)\n",
      "('Usually', 103)\n",
      "('Venetian', 104)\n",
      "('Victor', 105)\n",
      "('Was', 106)\n",
      "('We', 107)\n",
      "('Well', 108)\n",
      "('What', 109)\n",
      "('When', 110)\n",
      "('Why', 111)\n",
      "('Yes', 112)\n",
      "('You', 113)\n",
      "('_', 114)\n",
      "('a', 115)\n",
      "('abdication', 116)\n",
      "('able', 117)\n",
      "('about', 118)\n",
      "('above', 119)\n",
      "('abruptly', 120)\n",
      "('absolute', 121)\n",
      "('absorbed', 122)\n",
      "('absurdity', 123)\n",
      "('academic', 124)\n",
      "('accuse', 125)\n",
      "('accustomed', 126)\n",
      "('across', 127)\n",
      "('activity', 128)\n",
      "('add', 129)\n",
      "('added', 130)\n",
      "('admirers', 131)\n",
      "('adopted', 132)\n",
      "('adulation', 133)\n",
      "('advance', 134)\n",
      "('aesthetic', 135)\n",
      "('affect', 136)\n",
      "('afraid', 137)\n",
      "('after', 138)\n",
      "('afterward', 139)\n",
      "('again', 140)\n",
      "('ago', 141)\n",
      "('ah', 142)\n",
      "('air', 143)\n",
      "('alive', 144)\n",
      "('all', 145)\n",
      "('almost', 146)\n",
      "('alone', 147)\n",
      "('along', 148)\n",
      "('always', 149)\n",
      "('am', 150)\n",
      "('amazement', 151)\n",
      "('amid', 152)\n",
      "('among', 153)\n",
      "('amplest', 154)\n",
      "('amusing', 155)\n",
      "('an', 156)\n",
      "('and', 157)\n",
      "('another', 158)\n",
      "('answer', 159)\n",
      "('answered', 160)\n",
      "('any', 161)\n",
      "('anything', 162)\n",
      "('anywhere', 163)\n",
      "('apparent', 164)\n",
      "('apparently', 165)\n",
      "('appearance', 166)\n",
      "('appeared', 167)\n",
      "('appointed', 168)\n",
      "('are', 169)\n",
      "('arm', 170)\n",
      "('arm-chair', 171)\n",
      "('arm-chairs', 172)\n",
      "('arms', 173)\n",
      "('art', 174)\n",
      "('articles', 175)\n",
      "('artist', 176)\n",
      "('as', 177)\n",
      "('aside', 178)\n",
      "('asked', 179)\n",
      "('at', 180)\n",
      "('atmosphere', 181)\n",
      "('atom', 182)\n",
      "('attack', 183)\n",
      "('attention', 184)\n",
      "('attitude', 185)\n",
      "('audacities', 186)\n",
      "('away', 187)\n",
      "('awful', 188)\n",
      "('axioms', 189)\n",
      "('azaleas', 190)\n",
      "('back', 191)\n",
      "('background', 192)\n",
      "('balance', 193)\n",
      "('balancing', 194)\n",
      "('balustraded', 195)\n",
      "('basking', 196)\n",
      "('bath-rooms', 197)\n",
      "('be', 198)\n",
      "('beaming', 199)\n",
      "('bean-stalk', 200)\n",
      "('bear', 201)\n",
      "('beard', 202)\n",
      "('beauty', 203)\n",
      "('became', 204)\n",
      "('because', 205)\n",
      "('becoming', 206)\n",
      "('bed', 207)\n",
      "('been', 208)\n",
      "('before', 209)\n",
      "('began', 210)\n",
      "('begun', 211)\n",
      "('behind', 212)\n",
      "('being', 213)\n",
      "('believed', 214)\n",
      "('beneath', 215)\n",
      "('bespoke', 216)\n",
      "('better', 217)\n",
      "('between', 218)\n",
      "('big', 219)\n",
      "('bits', 220)\n",
      "('bitterness', 221)\n",
      "('blocked', 222)\n",
      "('born', 223)\n",
      "('borne', 224)\n",
      "('boudoir', 225)\n",
      "('bravura', 226)\n",
      "('break', 227)\n",
      "('breaking', 228)\n",
      "('breathing', 229)\n",
      "('bric-a-brac', 230)\n",
      "('briefly', 231)\n",
      "('brings', 232)\n",
      "('bronzes', 233)\n",
      "('brought', 234)\n",
      "('brown', 235)\n",
      "('brush', 236)\n",
      "('bull', 237)\n",
      "('business', 238)\n",
      "('but', 239)\n",
      "('buying', 240)\n",
      "('by', 241)\n",
      "('called', 242)\n",
      "('came', 243)\n",
      "('can', 244)\n",
      "('canvas', 245)\n",
      "('canvases', 246)\n",
      "('cards', 247)\n",
      "('care', 248)\n",
      "('career', 249)\n",
      "('caught', 250)\n",
      "('central', 251)\n",
      "('chair', 252)\n",
      "('chap', 253)\n",
      "('characteristic', 254)\n",
      "('charming', 255)\n",
      "('cheap', 256)\n",
      "('check', 257)\n",
      "('cheeks', 258)\n",
      "('chest', 259)\n",
      "('chimney-piece', 260)\n",
      "('chucked', 261)\n",
      "('cigar', 262)\n",
      "('cigarette', 263)\n",
      "('cigars', 264)\n",
      "('circulation', 265)\n",
      "('circumstance', 266)\n",
      "('circus-clown', 267)\n",
      "('claimed', 268)\n",
      "('clasping', 269)\n",
      "('clear', 270)\n",
      "('cleverer', 271)\n",
      "('close', 272)\n",
      "('clue', 273)\n",
      "('coat', 274)\n",
      "('collapsed', 275)\n",
      "('colour', 276)\n",
      "('come', 277)\n",
      "('comfortable', 278)\n",
      "('coming', 279)\n",
      "('companion', 280)\n",
      "('compared', 281)\n",
      "('complex', 282)\n",
      "('confident', 283)\n",
      "('congesting', 284)\n",
      "('conjugal', 285)\n",
      "('constraint', 286)\n",
      "('consummate', 287)\n",
      "('contended', 288)\n",
      "('continued', 289)\n",
      "('corner', 290)\n",
      "('corrected', 291)\n",
      "('could', 292)\n",
      "('couldn', 293)\n",
      "('count', 294)\n",
      "('countenance', 295)\n",
      "('couple', 296)\n",
      "('course', 297)\n",
      "('covered', 298)\n",
      "('craft', 299)\n",
      "('cried', 300)\n",
      "('crossed', 301)\n",
      "('crowned', 302)\n",
      "('crumbled', 303)\n",
      "('cry', 304)\n",
      "('cured', 305)\n",
      "('curiosity', 306)\n",
      "('curious', 307)\n",
      "('current', 308)\n",
      "('curtains', 309)\n",
      "('d', 310)\n",
      "('dabble', 311)\n",
      "('damask', 312)\n",
      "('dark', 313)\n",
      "('dashed', 314)\n",
      "('day', 315)\n",
      "('days', 316)\n",
      "('dead', 317)\n",
      "('deadening', 318)\n",
      "('dear', 319)\n",
      "('deep', 320)\n",
      "('deerhound', 321)\n",
      "('degree', 322)\n",
      "('delicate', 323)\n",
      "('demand', 324)\n",
      "('denied', 325)\n",
      "('deploring', 326)\n",
      "('deprecating', 327)\n",
      "('deprecatingly', 328)\n",
      "('desire', 329)\n",
      "('destroyed', 330)\n",
      "('destruction', 331)\n",
      "('desultory', 332)\n",
      "('detail', 333)\n",
      "('diagnosis', 334)\n",
      "('did', 335)\n",
      "('didn', 336)\n",
      "('died', 337)\n",
      "('dim', 338)\n",
      "('dimmest', 339)\n",
      "('dingy', 340)\n",
      "('dining-room', 341)\n",
      "('disarming', 342)\n",
      "('discovery', 343)\n",
      "('discrimination', 344)\n",
      "('discussion', 345)\n",
      "('disdain', 346)\n",
      "('disdained', 347)\n",
      "('disease', 348)\n",
      "('disguised', 349)\n",
      "('display', 350)\n",
      "('dissatisfied', 351)\n",
      "('distinguished', 352)\n",
      "('distract', 353)\n",
      "('divert', 354)\n",
      "('do', 355)\n",
      "('doesn', 356)\n",
      "('doing', 357)\n",
      "('domestic', 358)\n",
      "('don', 359)\n",
      "('done', 360)\n",
      "('donkey', 361)\n",
      "('down', 362)\n",
      "('dozen', 363)\n",
      "('dragged', 364)\n",
      "('drawing-room', 365)\n",
      "('drawing-rooms', 366)\n",
      "('drawn', 367)\n",
      "('dress-closets', 368)\n",
      "('drew', 369)\n",
      "('dropped', 370)\n",
      "('each', 371)\n",
      "('earth', 372)\n",
      "('ease', 373)\n",
      "('easel', 374)\n",
      "('easy', 375)\n",
      "('echoed', 376)\n",
      "('economy', 377)\n",
      "('effect', 378)\n",
      "('effects', 379)\n",
      "('efforts', 380)\n",
      "('egregious', 381)\n",
      "('eighteenth-century', 382)\n",
      "('elbow', 383)\n",
      "('elegant', 384)\n",
      "('else', 385)\n",
      "('embarrassed', 386)\n",
      "('enabled', 387)\n",
      "('end', 388)\n",
      "('endless', 389)\n",
      "('enjoy', 390)\n",
      "('enlightenment', 391)\n",
      "('enough', 392)\n",
      "('ensuing', 393)\n",
      "('equally', 394)\n",
      "('equanimity', 395)\n",
      "('escape', 396)\n",
      "('established', 397)\n",
      "('etching', 398)\n",
      "('even', 399)\n",
      "('event', 400)\n",
      "('ever', 401)\n",
      "('everlasting', 402)\n",
      "('every', 403)\n",
      "('exasperated', 404)\n",
      "('except', 405)\n",
      "('excuse', 406)\n",
      "('excusing', 407)\n",
      "('existed', 408)\n",
      "('expected', 409)\n",
      "('exquisite', 410)\n",
      "('exquisitely', 411)\n",
      "('extenuation', 412)\n",
      "('exterminating', 413)\n",
      "('extracting', 414)\n",
      "('eye', 415)\n",
      "('eyebrows', 416)\n",
      "('eyes', 417)\n",
      "('face', 418)\n",
      "('faces', 419)\n",
      "('fact', 420)\n",
      "('faded', 421)\n",
      "('failed', 422)\n",
      "('failure', 423)\n",
      "('fair', 424)\n",
      "('faith', 425)\n",
      "('false', 426)\n",
      "('familiar', 427)\n",
      "('famille-verte', 428)\n",
      "('fancy', 429)\n",
      "('fashionable', 430)\n",
      "('fate', 431)\n",
      "('feather', 432)\n",
      "('feet', 433)\n",
      "('fell', 434)\n",
      "('fellow', 435)\n",
      "('felt', 436)\n",
      "('few', 437)\n",
      "('fewer', 438)\n",
      "('finality', 439)\n",
      "('find', 440)\n",
      "('fingers', 441)\n",
      "('first', 442)\n",
      "('fit', 443)\n",
      "('fitting', 444)\n",
      "('five', 445)\n",
      "('flash', 446)\n",
      "('flashed', 447)\n",
      "('florid', 448)\n",
      "('flowers', 449)\n",
      "('fluently', 450)\n",
      "('flung', 451)\n",
      "('follow', 452)\n",
      "('followed', 453)\n",
      "('fond', 454)\n",
      "('footstep', 455)\n",
      "('for', 456)\n",
      "('forced', 457)\n",
      "('forcing', 458)\n",
      "('forehead', 459)\n",
      "('foreign', 460)\n",
      "('foreseen', 461)\n",
      "('forgive', 462)\n",
      "('forgotten', 463)\n",
      "('form', 464)\n",
      "('formed', 465)\n",
      "('forming', 466)\n",
      "('forward', 467)\n",
      "('fostered', 468)\n",
      "('found', 469)\n",
      "('foundations', 470)\n",
      "('fragment', 471)\n",
      "('fragments', 472)\n",
      "('frame', 473)\n",
      "('frames', 474)\n",
      "('frequently', 475)\n",
      "('friend', 476)\n",
      "('from', 477)\n",
      "('full', 478)\n",
      "('fullest', 479)\n",
      "('furiously', 480)\n",
      "('furrowed', 481)\n",
      "('garlanded', 482)\n",
      "('garlands', 483)\n",
      "('gave', 484)\n",
      "('genial', 485)\n",
      "('genius', 486)\n",
      "('gesture', 487)\n",
      "('get', 488)\n",
      "('getting', 489)\n",
      "('give', 490)\n",
      "('given', 491)\n",
      "('glad', 492)\n",
      "('glanced', 493)\n",
      "('glimpse', 494)\n",
      "('gloried', 495)\n",
      "('glory', 496)\n",
      "('go', 497)\n",
      "('going', 498)\n",
      "('gone', 499)\n",
      "('good', 500)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4af1d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d93df5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8568d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faaac92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85227323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2026.1.15-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n",
      "Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2026.1.15-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, tiktoken\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [tiktoken]\n",
      "\u001b[1A\u001b[2KSuccessfully installed regex-2026.1.15 tiktoken-0.12.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f35f28d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85bcb259",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2097c711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
      "20\n",
      "[1169, 271, 262, 271]\n",
      "[1169, 19153, 262, 30, 220]\n",
      "[1169, 30]\n",
      "theis theis\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)\n",
    "print(len(integers))\n",
    "\n",
    "text2= \"theis theis\"\n",
    "text3= \"the ?? the? \"\n",
    "text4= \"the?\"\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "integers2 = tokenizer.encode(text2, allowed_special={\"<|endoftext|>\"})\n",
    "integers3 = tokenizer.encode(text3, allowed_special={\"<|endoftext|>\"})\n",
    "integers4 = tokenizer.encode(text4, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers2)\n",
    "print(integers3)\n",
    "print(integers4)\n",
    "\n",
    "strings = tokenizer.decode(integers2)\n",
    "\n",
    "print(strings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd59c9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b26638c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
